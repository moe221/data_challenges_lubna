{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetune your Neural Network and save it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üéØ Goals of this challenge:\n",
    "    \n",
    "1. ‚öôÔ∏è **Finetune the optimizer** of a neural network\n",
    "2. üíæ **Save**/**Load** a trained neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üë©üèª‚Äçüè´ Now that you have solid foundations about what Neural Networks are, how to design their architecture and how to prevent them from overfitting, let's take a closer look at the **`.compile(loss = ..., metrics = ..., activation = ...)`** part."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (0) The Boston Housing Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üìö ***Tensorflow.Keras*** has several built-in dataset that you can find [here](https://www.tensorflow.org/api_docs/python/tf/keras/datasets)\n",
    "\n",
    "üè† Among them, we are going to use the **`Boston Housing Dataset`**:\n",
    "- our mission is to ***predict the values of the houses in k USD***\n",
    "- and we will measure the performance of our model (s) using the _Mean Absolute Error (MAE)_ metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Data Visualisation\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (0.1) Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Boston Housing Dataset from Keras\n",
    "from tensorflow.keras.datasets import boston_housing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the dataset:\n",
    "(X_train, y_train), (X_test, y_test) = boston_housing.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shape of the dataset: \n",
    "(X_train.shape, y_train.shape), (X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (0.2) Quick glance at the target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of the houses' prices in the training set\n",
    "sns.histplot(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Null values and types of each feature:\n",
    "pd.DataFrame(X_train).info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistics about the numerical columns\n",
    "pd.DataFrame(X_train).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (0.3) Minimal data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üëâ Here, we don't have any duplicates or missing values. Let's do the strict minimum of data preprocessing, _i.e._ ***scaling*** and move on quickly to the modelling phase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question: scaling your features** ‚ùì \n",
    "\n",
    "Standardize `X_train` and `X_test`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (0.4) Baseline Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "üßëüèª‚Äçüè´ In a regression task, **the baseline model always predicts the average value of `y_train`**\n",
    "\n",
    "<details>\n",
    "    <summary><i>Really ? </i></summary>\n",
    "    \n",
    "* üêí Yes in most cases...! we also like to call it _\"dumbbest model\"_ but _\"baseline model\"_ is more correct politically speaking :)\n",
    "* ‚ùóÔ∏è Be aware that this is not the only possible way of building a baseline model.\n",
    "* üíπ ***In Time Series, the baseline model consists in predicting the last seen value***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question: what would be the performance of the baseline model here ?** ‚ùì \n",
    "\n",
    "Before running any Machine Learning algorithm or advanced Deep Learning Neural Networks, it would be great to establish a benchmark score that you are supposed to beat. Otherwise, what is the point of running a fancy algorithm if you cannot beat this benchmark score on the test set (other than showing off) ? \n",
    "\n",
    "* Compute the Mean Absolute Error on the test set if your dumb prediction corresponds to the mean value of `y_train` computed on the train set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (1) The Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question: initializing a Neural Network with a specific architecture** ‚ùì\n",
    "\n",
    "Write a function **`initialize_model`** that generates a Neural Network with 3 layers: \n",
    "- Input layer: 10 neurons, _\"relu\"_ activation function, and an the appropriate input dimension\n",
    "- Hidden layer: 7 neurons and the _\"relu\"_ activation function\n",
    "- Predictive layer: an appropriate layer corresponding to the problem we are trying to solve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "def initialize_model():\n",
    "    pass  # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question: number of parameters** ‚ùì\n",
    "\n",
    "How many parameters do have in this model ? \n",
    "1. Compute this number yourself\n",
    "2. Double-check your answer with _model.summary()_\n",
    "\n",
    "We already covered this question about the **number of parameters in a Fully Connected/Dense network** during **Deep Learning > 01. Fundamentals of Deep Learning** but it is always good to make sure you master the foundations of a new discipline üòâ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary><i>Answer</i></summary>\n",
    "    \n",
    "* Each house has `X_train.shape[-1]` = 13 features\n",
    "* Remember that a neuron is a linear activation combined with an activation so we will have 13 weights and 1 bias\n",
    "\n",
    "1. First layer : 10 neurons $\\times$ (13 weights + 1 bias ) = 140 params\n",
    "2. Second layer : 7 neurons $\\times$ (10 weights + 1 bias ) = 77 params\n",
    "3. Third layer : 1 neuron $\\times$ (7 weights + 1 bias) = 8 params\n",
    "    \n",
    "    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2) The optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "‚ùì **Question about the compiling method** ‚ùì \n",
    "\n",
    "Write a function that :\n",
    "* takes as arguments _both_ a _model_ and an _optimizer_, \n",
    "* ***compiles*** the model,\n",
    "* and returns the compiled model\n",
    "\n",
    "Please select wisely:\n",
    "* the _loss function_ to be optimized \n",
    "* and  the _metrics_ on which the model should be evaluated ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_model(model, optimizer_name):\n",
    "    \n",
    "    pass  # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question: evaluating the model** ‚ùì \n",
    "\n",
    "- Initialize the model and compile it with the `adam` optimizer \n",
    "- Fit it on the training data. \n",
    "- Evaluate your model on the test data.\n",
    "\n",
    "Don't forget to use an Early Stopping criterion to avoid overfitting \n",
    "\n",
    "<details>\n",
    "    <summary><i>Notes</i></summary>\n",
    "\n",
    "As we saw in the _\"How to prevent overfitting\" challenge_,  you could also use L2 penalties and Dropout Layers to prevent overfitting but:\n",
    "1. _Early Stopping is the easiest and quickest code to implement, you just declare _es = EarlyStopping(...)_ and you call it back in the _.fit()_ _\n",
    "2. _The main goal of this challenge is to understand the impact of the optimizer_, so stay focused üòâ\n",
    "    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "pass  # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question: a Neural Network vs. a baseline** ‚ùì \n",
    "\n",
    "Compare the MAE on the test best between this Neural Network and the baseline model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mae_test_baseline = mean_absolute_error_test_baseline\n",
    "mae_test_neuralnet = res\n",
    "\n",
    "print(f\"The MAE on the test is {mae_test_neuralnet:.4f} for the Neural Network vs. {mae_test_baseline:.4f} for the baseline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2.1) Which optimizer is the best ? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "‚ùì **Question: trying out different optimizers...** ‚ùì \n",
    "\n",
    "Re-run the same model,  on the same data, but using different optimizers (in a `for` loop). \n",
    "\n",
    "For each optimizer:\n",
    "- üìâ Plot the history of the Loss (MSE) and the Metrics (MAE)\n",
    "    - üéÅ We coded two functions `plot_loss_mae` and `plot_loss_mse`. Which one should you use ? Feel free to use it.\n",
    "- ‚úçÔ∏è Report the corresponding Mean Absolute Error\n",
    "- ‚è≥ Compute the time your Neural Net needed to fit the training set\n",
    "\n",
    "üìö [**tensorflow/keras/optimizers**](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss_mae(history):\n",
    "    fig, (ax1, ax2) = plt.subplots(1,2, figsize=(13,4))\n",
    "    ax1.plot(history.history['loss'])\n",
    "    ax1.plot(history.history['val_loss'])\n",
    "    ax1.set_title('Model loss')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylim(ymin=0, ymax=200)\n",
    "    ax1.legend(['Train', 'Validation'], loc='best')\n",
    "    ax1.grid(axis=\"x\",linewidth=0.5)\n",
    "    ax1.grid(axis=\"y\",linewidth=0.5)    \n",
    "    \n",
    "    ax2.plot(history.history['mae'])\n",
    "    ax2.plot(history.history['val_mae'])\n",
    "    ax2.set_title('MAE')\n",
    "    ax2.set_ylabel('MAE')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylim(ymin=0, ymax=20)\n",
    "    ax2.legend(['Train', 'Validation'], loc='best')\n",
    "    ax2.grid(axis=\"x\",linewidth=0.5)\n",
    "    ax2.grid(axis=\"y\",linewidth=0.5)    \n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def plot_loss_mse(history):\n",
    "    fig, (ax1, ax2) = plt.subplots(1,2, figsize=(13,4))\n",
    "    ax1.plot(history.history['loss'])\n",
    "    ax1.plot(history.history['val_loss'])\n",
    "    ax1.set_title('Model loss')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylim(ymin=0, ymax=20)\n",
    "    ax1.legend(['Train', 'Validation'], loc='best')\n",
    "    ax1.grid(axis=\"x\",linewidth=0.5)\n",
    "    ax1.grid(axis=\"y\",linewidth=0.5)    \n",
    "\n",
    "    \n",
    "    ax2.plot(history.history['mse'])\n",
    "    ax2.plot(history.history['val_mse'])\n",
    "    ax2.set_title('MSE')\n",
    "    ax2.set_ylabel('MSE')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylim(ymin=0, ymax=200)\n",
    "    ax2.legend(['Train', 'Validation'], loc='best')\n",
    "    ax2.grid(axis=\"x\",linewidth=0.5)\n",
    "    ax2.grid(axis=\"y\",linewidth=0.5)    \n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question: a Neural Network vs a baseline - part 2** ‚ùì \n",
    "\n",
    "Are your predictions better than the benchmark model you've evaluated at the beginning of the notebook?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary><i>Answer</i></summary>\n",
    "    \n",
    "You can see that the Neural Network beat the baseline when using either _adam_ or _optimizer_ but it the result was worse than the baseline with the _adagrad_ optimizer.\n",
    " \n",
    "    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "üëá The advice from the Deep Learning community is the following üëá:\n",
    "\n",
    "* üî• So far, our most performant optimizer is **`adam`**. Maybe a mathematician specialised in numerical methods will find a better solver in the future but for the moment..., _\"adam\"_ is your best friend and it has already been helping us achieve remarkable results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "‚ùóÔ∏èSo, what's next in this challenge ? ‚ùóÔ∏è\n",
    "\n",
    "üë©‚Äçüéì Do you remember the ***Machine Learning > 04. Under The Hood*** where we coded our ***Gradient Descent*** choosing a specific ***learning rate*** ? It represents how slow/fast your algorithm learns. In other words, it controls the intensity of the change of the weights at each optimization of the neural network, at each backpropagation!\n",
    "\n",
    "üöÄ Well, the ***solvers in Machine Learning*** and the ***optimizers in Deep Learning*** are advanced iterative methods relying on ***hyperparameters*** and the learning rate is one of them!\n",
    "\n",
    "ü§î How can I control this learning rate ? \n",
    "\n",
    "‚úÖ Instead of calling an optimizer with a string (\"adam\", \"rmsprop\", ...) which uses a default value of the learning rate, we will call üìö [**tf/keras/optimizers**](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers) objects üìö and tailor them to our needs.\n",
    "\n",
    "üßëüèª‚Äçüè´ Different learning rates have different consequences, as shown here : "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"learning_rate.png\" alt=\"Learning rate\" style=\"height:300px;\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2.2) The influence of the Learning Rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "‚ùì **Question: selecting an optimizer with a custom learning rate** ‚ùì \n",
    "\n",
    "üìö [**tf/keras/optimizers/Adam**](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adam)\n",
    "\n",
    "Instead of initializing the optimizer with a string, let's initialize a real optimizer directly.\n",
    "\n",
    "* Instantiate an Adam optimizer with a learning rate of $ \\alpha = 0.1$\n",
    "    * Keep the other values to their default values. \n",
    "* Use this optimizer in the `compile_model` function\n",
    "* Train/Fit the model\n",
    "* Plot the history\n",
    "* Evaluate your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question: playing with learning rates** ‚ùì \n",
    "\n",
    "Now, reproduce the same plots and results but for different learning rates.\n",
    "\n",
    "<details>\n",
    "    <summary><i>Remark</i></summary>\n",
    "\n",
    "There is a chance that the y-axis is too large for you to visualize some results with some learning rates. In that case, feel free to re-write the plot function to plot only the epochs $> 10$.\n",
    "    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rates = [0.0001, 0.001, 0.01, 0.1, 1, 2]\n",
    "results = []\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    \n",
    "    pass  # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (3) The loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "‚ùóÔ∏è It is important to **clearly understand the different between losses and metrics**. ‚ùóÔ∏è\n",
    "\n",
    "* üèãüèª‚Äç‚ôÄÔ∏è The **loss functions** are computed ***during the training procedure***\n",
    "    - For Regression tasks, the classic loss functions are : (Root) Mean Squared Error ((R)MSE), Mean Absolute Error (MAE), Mean Squared Logarithmic Error (MSLE as seen during the Kaggle Challenge)\n",
    "    - For Classification tasks, the classic loss functions are : Binary Crossentropy (also known as LogLoss), the Categorical Crossentropy, the Hinge Loss, ...\n",
    "* üßëüèª‚Äçüè´ The **metrics** are computed ***to evaluate your models, after training them*** !\n",
    "    - For Regression tasks, common metrics are : MSE, MAE, RMSE, Coefficient of Determination R2, ...\n",
    "    - For Classification tasks, common metrics are : Accuracy, Recall, Precision, F1-Score\n",
    "* üëÄ Notice that some metrics can also be used as loss functions... as long as they are differentiable ! (e.g. the *MSE*)\n",
    "\n",
    "If these notions are not clear, we strongly advise to review ***Machine Learning > 03. Performance Metrics*** and ***Machine Learning > 05. Model Tuning***\n",
    "\n",
    "---\n",
    "\n",
    "‚è© Alright, after this reminder, let's move on:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question: optimizing a model with respect to a certain loss function** ‚ùì \n",
    "\n",
    "* Run the same neural network, once with the `mae` as the loss, and once with the `mse`.  \n",
    "* In both case, compare `mae_train`, `mae_val`, `mse_train`, `mse_val` and conclude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ü§î When you work on this regression task, you want to achieve the lowest MAE in the test set at the end, right ? So why wouldn't we use it directly as a loss function that would decrease with respect to the number of epochs ? \n",
    "\n",
    "<details>\n",
    "    <summary>Answer</summary>\n",
    "\n",
    "Well, even the Deep Learning research community is still trying to answer these types of questions rigorously.\n",
    "    \n",
    "One thing for sure: In Deep Learning, you will never really reach the \"global minimum\" of the true loss function (the one computed using your entire training set as one single \"batch\"). So, in your first model (minimizing the MAE loss), your global MAE minimum has clearly **not** been reached (otherwise you could never beat it). \n",
    "\n",
    "Why? It may well be that the minimization process of the second model has performed better. Maybe because the loss function \"energy map\" is \"smoother\" or more \"convex\" in the case of MSE loss? Or maybe your hyper-parameter are best suited to the MSE than to the MAE loss?\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (4)  Saving and loading a trained neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ü§Ø Imagine that you trained a complex Neural Network (many layers/neurons) on a huge dataset. The parameters of your Deep Learning Model (weights and biases) are now optimized and you would like to share these weights with a teammate who wants to predict a new datapoint. Would you give this person your notebook for her/him to run it entirely and then predict the new datapoint ? Hell no, we have a much better solution:\n",
    "- üíæ Save the weights of the optimised neural network\n",
    "- ü§ù Your friend/colleague/teammate/classmate can use them to predict a new datapoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question: training a good model** ‚ùì\n",
    "\n",
    "* Try to reach a MAE on the test set that is lower than 5 (_feel free to re-create the architecture and redefine your compiling parameters in this section!_)\n",
    "    - _Remember: we are predicting house prices, so a mistake of less than 5_000 USD is already good in the real estate industry)_\n",
    "    \n",
    "* Whether you managed to reach it or not, move on to the question _\"saving a model\"_ after a few attempts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbresult import ChallengeResult\n",
    "result = ChallengeResult('solution',\n",
    "    mae_test = mae_test)\n",
    "\n",
    "result.write()\n",
    "print(result.check())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üìö [**tf/keras/models/save_model**](https://www.tensorflow.org/api_docs/python/tf/keras/models/save_model).\n",
    "\n",
    "‚ùì **Question: saving a model** ‚ùì  \n",
    "\n",
    "Save your model using `.save_model(model, 'name_of_my_model')` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üìö [**tf/keras/models/load_model**](https://www.tensorflow.org/api_docs/python/tf/keras/models/load_model)\n",
    "\n",
    "‚ùì **Question: Loading a model** ‚ùì \n",
    "\n",
    "* Load the model that you've just saved using `.load_model('name_of_your_model')` and store it into a variable called`loaded_model\n",
    "\n",
    "* Evaluate it on the test data to check that it gives the same result as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (5) (Optional) Exponential Decay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùóÔ∏è Warning ‚ùóÔ∏è \n",
    "\n",
    "* This section is optional and for advanced practionners\n",
    "* The next question is not essential and can be indeed skipped as many algorithms can be run without such optimization. \n",
    "\n",
    "üßëüèª‚Äçüè´ Instead of keeping a fixed learning rate, you can change it from one iteration to the other, with the intuition that at first, you need a large learning rate to learn fast, and as the neural network converges and gets closer to the minimum of the loss function, you can decrease the value of the learning rate. This is called a **`scheduler`**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question: the Exponential Decay Scheduler** ‚ùì \n",
    "\n",
    "* Use the üìö [Exponential Decay Scheduler](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/schedules/ExponentialDecay) üìö in the `adam` optimizer\n",
    "* Run it on the previous data. \n",
    "\n",
    "Start with the following:\n",
    "\n",
    "```python\n",
    "initial_learning_rate = 0.001 # start with default Adam value\n",
    "\n",
    "lr_schedule = ExponentialDecay(\n",
    "    # Every 5000 iterations, multiply the learning rate by 0.7\n",
    "    initial_learning_rate, decay_steps = 5000, decay_rate = 0.7,\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
    "\n",
    "pass  # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üèÅ Congratulations !\n",
    "\n",
    "üíæ Do not forget to `git add/commit/push` your notebook...\n",
    "\n",
    "üöÄ ... and move to the next challenge !"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
